---
title: "Practica 1"
author: "Guillermo Palomo y Miguel Díaz-Plaza"
date: "`r Sys.Date()`"
output:
  word_document:
    reference_docx: Mystyleword.docx
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE,warning=FALSE}

library(mlr3data)
library(skimr)
library(corrplot)
library(janitor)
library(mlr3)
library(mlr3learners)
#remotes::install_github("mlr-org/mlr3extralearners")
library(mlr3extralearners)
library(mlr3pipelines)
library(kknn)
library(rpart)
library(BBmisc)
library(Cubist)
library(mlr3verse)
library(xgboost)
library(ranger)
library(randomForest)
library(gbm)
library(bbotk)
library(mlr3hyperband)
```

\pagebreak

# Introducción

El objetivo de esta práctica será predecir la radiación solar diaria en una planta solar de Oklahoma a partir de predicciones de variables meteorológicas del día anterior. Para ello disponemos de una base de datos que explica el comportamiento de la radiación  solar a partir de distintas variables meteorológicas, en una central solar, en nuestro caso la **número 9**.

Los atributos de entrada que tenemos son las predicciones del día siguiente de 15 variables meteorológicas. Los datos han sido generados por una simulación de 
ecuaciones de la atmósfera denominada NWP (Numerical Weather Prediction).

Dichas variables han sido generadas para cinco momentos del día siguiente: 12h, 15h, 18h, 21h, 24h UTC.

Como hay 15 variables, generadas para 5 momentos del día siguiente, en total tenemos 75 atributos de entrada.

La última columna de los datos se denomina "salida" y es la radiación solar acumulada durante todo el día.

Poseemos un conjunto de datos desde 1994-2003. Con ellos, haremos varias pruebas para obtener el mejor modelo posible. Esto incluye todo tipo de pruebas sobre actividades de la metodología: ¿cuál es el mejor método para preprocesar?, ¿cuál es el mejor método de construcción de modelos?, ¿cuáles son los mejores hiper-parámetros para cada método? etc. Obtendremos una estimación del comportamiento de lo que podría obtener el modelo y construiremos el modelo final.

En todo nuestro trabajo utilizaremos la semilla `set.seed(100430523)`.

\pagebreak

# 1. EDA

El primer paso que realizaremos será un Análisis Exploratorio de los Datos (EDA).

Cargamos las dos bases de datos que tenemos, en esta primera parte utilizaremos `datos_disp` para crear nuestro modelo final y ese modelo final para calcular predicciones sobre el conjunto`datos_compet`.

```{r}

datos_disp = readRDS("disp_9.rds")
datos_compet = readRDS("compet_9.rds")

```

## a. skim

```{r}

skim(datos_disp)

```

Observamos un total de **$4380$ instancias** y **$76$ atributos ($75$ más la salida)**. 

## b. str

```{r}

str(datos_disp)

```

La mayoría ($43$) son variables categóricas ordinales con niveles, es decir, factores "low", "medium" y "high". También hay algunas numéricas ($24$) y otras categóricas ($9$) que str detecta como characters que son las que tienen colores (red, blue y green) como observaciones, y debería detectarlas como factores, paso que realizaremos más adelante en la imputación .

## c. Missing values (NA's)

Aparecen valores faltantes para todo tipo de variables en la base de datos. Un elevado número de NA's nos puede dar indicios de eliminar dicha variable, ya que habría que predecir una gran proporción de datos de dicha variable en caso de querer utilizarla.

```{r}
noms <- colnames(datos_disp)
Nas <- round(1 - skim(datos_disp)$complete_rate,3)
df_NA <- data.frame(noms,Nas)
df_NA

```

La tabla anterior muestra la proporción de NAs. Como vemos la mayoría de atributos están completos pero llaman la atención **"ulwrf_s4_1"** y **"ulwrf_s5_1"** con **más de un 90% de NA's**


## d. Atributos constantes

En ocasiones nos encontramos con atributos que solo toman un valor para toda la población. Esto hace que sean irrelevantes por lo que es conveniente eliminarlos.

Como vemos en el skim del apartado a, hay algunas variables constantes que son todo ceros, por lo que las eliminaremos después, en el preproceso, junto con las que tengan más del 80% de NAs.

También el skim nos ofrece histogramas que nos pueden ser útiles para localizar estos atributos constantes.

Los eliminaremos más adelante con la función `remove_constant()` de la librería janitor.


## e. Plot de la variable de respuesta a lo largo del tiempo.

```{r}
serie <- ts(datos_disp$salida, freq=365, start = c(1992,1))
plot(serie, main="Serie temporal de la radiación solar(1992-2003)",
xlab="Año", ylab="Radiación solar")
```

Vemos que la radiación solar oscila estacionalmente. En los meses de verano encontramos los valores más altos, porque hay radiación más alta, y en invierno los más bajos, porque en los meses de invierno hay radiación más baja. 

Observamos además 12 distintos ciclos, que corresponden al número de años en los que se recogen los datos de la base de datos.

# 2. RAE

La métrica del "Relative absolute error" (RAE) se basa en comparar el rendimiento de un modelo trivial, por ejemplo, la media de la salida, con el modelo que estemos utilizando. 

De otra forma, el RAE es una medida de evaluación de modelos predictivos y es expresada como las predicciones de nuestro modelo menos las observaciones, entre la media de los datos menos las observaciones.

Su fórmula es:

$$\frac{|\hat{y}_1-y_1|+|\hat{y}_2-y_2|+...+|\hat{y}_p-y_p|}{|\bar{y}-y_1|+|\bar{y}-y_2|+...+|\bar{y}-y_p|}$$

Un modelo razonable tendrá un **RAE** menor que $1$ y serán mejores aquellos modelos cuyo **RAE** esté más próximo a $0$.

En MLR3, se simplifica bastante su cálculo y se obtiene utilizando:

```{r eval=FALSE}
medida <- msr("regr.rae")
```


# 3. Buscando el mejor modelo

## 3.1. Imputación y escalado

A continuación, vamos a comparar diferentes métodos de imputación de NA's y escalado de los datos, y vamos a determinar cuál es la mejor combinación. 

Existen un total de **$36$ combinaciones posibles**, ya que vamos a tener en cuenta:

- 3 tipos de imputación para variables categóricas: con la moda (imputemode), con un valor al azar (imputesample) y con el learner de clasificación de rpart (classif_learner_rpart)

- 3 tipos de imputación para variables numéricas o cuantitativas: con la media (imputemean), con la mediana (imputemedian) y con el learner de regresión de rpart (imputehist. regr_learner_rpart)

- 3 tipos de escalado: scale, scalerange y scalemaxabs.

Escogeremos un tipo de imputación para variables categóricas, otro tipo de imputación para variables cuantitativas y un tipo de escalado. Elegiremos aquella combinación de las tres con menor RAE.

Con ayuda del código de Carlos Morales y unos ligeros cambios que lo hacen más sencillo y automático encontramos la mejor forma de imputar y escalar.

En el preproceso sin info leakage, lo que hemos hecho es suprimir las columnas constantes, y aquellas con un porcentaje de NA's superior al $80\%$.

Por su parte, en el preproceso con info leakage, ya si es cuando tratamos de convertir predictores, y elegimos las formas de imputado y escalado óptimas para nuestros datos.

```{r}

# Preproceso no info leakage

datos_disp_RC <- remove_constant(datos_disp, na.rm = T, quiet = F)
datos_disp_RC_RNA <- datos_disp_RC[,!(names(datos_disp_RC) %in% noms[Nas >= 0.8])]

df <- datos_disp_RC_RNA

charcol <- c(colnames(df[, sapply(df, class) == 'character']))

for (i in charcol){
  df[[i]] <- as.factor(df[[i]])
}

c(colnames(df[, sapply(df, class) == 'character'])) #Ya no hay columnas que sea 'character', solo factores

source("extras_from_mlr.R")
df <- createDummyFeatures(df, target = "salida")


# Preproceso info leakage

imputcat <- c( "imputemode", "imputesample", "imputelearner")
imputnum <- c("imputemean", "imputemedian", "imputehist", "imputelearner")
esc <- c("scale", "scalerange", "scalemaxabs")

eshiper <- expand.grid(imputcat,imputnum, esc)

df_task <- as_task_regr(df, target = "salida")


knn_lrn <- lrn("regr.kknn")

res_desc <- rsmp("custom")
res_desc$instantiate(df_task,
  train = list(1:(6*365)), # 6 a?os
  test = list((6*365+1):(9*365))) # 3 a?os

medida <- msr("regr.rae")

totalerror <- c()
set.seed(100430523)

for (i in 1:dim(eshiper)[1]) {
  if (as.character(eshiper[i,1]) == "imputelearner" & as.character(eshiper[i,2]) != "imputelearner"){
    graph =
      po(as.character(eshiper[i,1]), lrn("classif.rpart")) %>>%
      po(as.character(eshiper[i,2])) %>>%
      po(as.character(eshiper[i,3])) %>>%
      po(knn_lrn)
  }
  else if (as.character(eshiper[i,2]) == "imputelearner" & as.character(eshiper[i,1]) != "imputelearner"){
    graph =
      po(as.character(eshiper[i,1])) %>>%
      po(as.character(eshiper[i,2]), lrn("regr.rpart")) %>>%
      po(as.character(eshiper[i,3])) %>>%
      po(knn_lrn)
  }
  else if (as.character(eshiper[i,2]) == "imputelearner" & as.character(eshiper[i,1]) == "imputelearner"){
    graph =
      po(as.character(eshiper[i,1]), lrn("classif.rpart"), id = "impute_clrpart") %>>%
      po(as.character(eshiper[i,2]), lrn("regr.rpart"), id = "impute_regrpart") %>>%
      po(as.character(eshiper[i,3])) %>>%
      po(knn_lrn)
  }
  else{
    graph =
      po(as.character(eshiper[i,1])) %>>%
      po(as.character(eshiper[i,2])) %>>%
      po(as.character(eshiper[i,3])) %>>%
      po(knn_lrn)
  }
  graph_lrn <- as_learner(graph)
  knn_resample <- resample(task = df_task,
                           learner = graph_lrn,
                           resampling = res_desc)
  knn_rae <- knn_resample$aggregate(medida)
  totalerror <- append(totalerror, knn_rae)
}  

nomconf <- eshiper[match(min(totalerror), totalerror),]
cat(paste("El menor error producido es: ", min(totalerror),
          "La mejor configuración de preprocesado será:" ,
          as.character(eshiper[match(min(totalerror), totalerror),1]),
          as.character(eshiper[match(min(totalerror), totalerror),2]),
          as.character(eshiper[match(min(totalerror), totalerror),3]),
          sep = "\n"))


```

La combinación ganadora es: **imputesample, imputemedian y scalerange**; con un error, que es el menor de entre las 36 combinaciones, de $0.446936555517012$.

Por tanto, el mejor preproceso de nuestros datos se consigue con una imputación de los NA's de las varibles categóricas según datos al azar, una imputación en las variables numéricas según la mediana, y un escalado según el rango.

## 3.2. Evaluación sin ajuste de hiper-parámetros

En este siguiente apartado compararemos varios métodos SIN ajuste de hiper-parámetros. Estos métodos son los siguientes: **regr.lm, rpart, vecino más cercano, cubist, y SVM lineal y radial (kernel gausiano)**.

Para todos los métodos citados, emplearemos la combinación ganadora de preproceso de los datos que vimos en el apartado anterior que era: **imputesample, imputemedian y scalerange**.

```{r message=FALSE, warning=FALSE}

set.seed(100430523)

t <- matrix(0,1,2)
colnames(t)=c("Learner","Rae")

metodos <- c("regr.lm","regr.rpart","regr.kknn","regr.cubist","regr.svm") 

df_task <- as_task_regr(df, target="salida")

res_desc=rsmp("custom")
res_desc$instantiate(df_task, 
                     train=list(1:(6*365)), 
                     test=list((6*365+1):(9*365)))

preproceso <- po("imputesample") %>>% po("imputemedian") %>>% po("scalerange") 


for (i in metodos) {
  learner = lrn(i)
  graph = preproceso %>>% po(learner)
  secuencia = as_learner(graph)
  metodo_resample = resample(task=df_task, 
                             learner=secuencia, 
                             resampling=res_desc)
  rae = metodo_resample$aggregate(msr("regr.rae"))
  t <- rbind(t, c(i,rae))
}

graph=preproceso %>>%
      po(lrn("regr.svm",kernel="radial"))
secuencia = as_learner(graph)
metodo_resample = resample(task=df_task, 
                           learner=secuencia, 
                           resampling=res_desc)
rae = metodo_resample$aggregate(msr("regr.rae"))
t <- rbind(t, c("radial.svm",rae))
t <- data.frame(t[-1,], row.names = NULL) 
t$Rae <- round(as.numeric(t$Rae),3)
t

```

En esta tabla observamos que el mejor método es el **cubist**, ya que es el que tiene menor RAE con $0.361$. Aunque, todos los modelos parecen válidos (unos mejores que otros, por ejemplo los peores parecen rpart y KNN) ya que tienen un error parecido y cercano al de cubist.


## 3.3. Evaluación con ajuste de hiper-parámetros

A continuación en este apartado, repetiremos el proceso del apartado anterior, pero en esta ocasión, en lugar de poner en omisión o default los hiper-parámetros, vamos a ajustarlos.

Los métodos que utilizaremos para calcular los mejores hiper-parámetros serán: 

- Para el vecino más cercano (**KNN**), vamos a ajustar sólo el número de vecinos con **Grid Search**.

- Para **rpart** y **SVM** usad **Random Search**.

- De cubist y de regr.lm no vamos a hacer ajuste de hiper-parámetros.

En este apartado, respecto al tipo de resample, hemos tenido que cambiar de "custom", que es el que estábamos usando hasta el momento a "holdoutorder" (manteniendo los 9 años de entrenamiento y los 3 de test). Nos hemos visto obligados a hacer este cambio porque de la otra manera no funcionaba el AutoTuner.

- KNN con Grid Search

```{r}

set.seed(100430523)
df_task <- as_task_regr(df, target="salida")

desc_outer = rsmp("custom")
desc_outer$instantiate(df_task, train=list(1:(6*365)), test=list((6*365+1):(9*365)))

source("ResamplingHoldoutOrder.R")
desc_inner = rsmp("holdoutorder", ratio=6/9)

kknn_space = ps(
  regr.kknn.k = p_fct(levels = seq(1,50,2))
)

generate_design_grid(kknn_space, param_resolutions = c(regr.kknn.k=10))

terminator = trm("none")

tuner = tnr("grid_search", param_resolutions=c(regr.kknn.k=10))

kknn_learner=preproceso %>>% po(lrn("regr.kknn"))

kknn_ajuste = AutoTuner$new(
  learner = kknn_learner,
  resampling = desc_inner,
  measure = msr("regr.rae"),
  search_space = kknn_space,
  terminator = terminator,
  tuner = tuner,
  store_tuning_instance = TRUE
)

lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

kknn_ajuste_resample = resample(df_task, kknn_ajuste, desc_outer, store_models = TRUE )

kknn_ajuste_rae = kknn_ajuste_resample$aggregate(msr("regr.rae"))
print(round(kknn_ajuste_rae,3))

```
 
Vemos como mejora el método KNN con el ajuste de hiper-parámetros, consiguiendo un RAE de $0.431$, frente al $0.454$ que obtuvimos sin el ajuste de hiper-parámetros.

- rpart con Random Search

```{r}

set.seed(100430523)

desc_outer = rsmp("custom")
desc_outer$instantiate(df_task, train=list(1:(6*365)), test=list((6*365+1):(9*365)))
desc_inner = rsmp("holdoutorder", ratio=6/9)
rpart_space = ps(
   regr.rpart.minsplit = p_int(lower=10, upper=20),  
   regr.rpart.maxdepth = p_int(lower = 2,upper = 6)
)
terminator = trm("evals", n_evals = 20)
tuner = tnr("random_search")
secuencia = preproceso %>>% po(lrn("regr.rpart"))

rpart_ajuste = AutoTuner$new(
  learner = secuencia,
  resampling = desc_inner,
  measure = msr("regr.rae"),
  search_space = rpart_space,
  terminator = terminator,
  tuner = tuner
)
rpart_ajuste_resample = resample(df_task, rpart_ajuste, desc_outer, store_models = T )

rae=rpart_ajuste_resample$aggregate(msr("regr.rae"))
print(round(rae,3))
```

En esta ocasión, el ajuste de hiper-parámetros no sirve para mejorar la precisión de rpart. Con ajuste de hiper-parámetros el RAE es de $0.448$ y sin ajuste era de $0.447$

- SVM con Random Search

```{r}

set.seed(100430523)

secuencia = preproceso %>>% po(lrn("regr.svm", kernel="radial", type = "eps-regression"))
svm_space = ps(
   regr.svm.cost = p_dbl(lower=-3, upper=3, trafo=function(x)10^x), 
   regr.svm.gamma = p_dbl(lower=-3, upper=3, trafo=function(x)10^x)
)
generate_design_random(svm_space, 50)
terminator = trm("evals", n_evals = 5)
tuner = tnr("random_search")
svm_ajuste = AutoTuner$new(
  learner = secuencia,
  resampling = desc_inner,
  measure = msr("regr.rae"),
  search_space = svm_space,
  terminator = terminator,
  tuner = tuner,
  store_tuning_instance = TRUE
)
svm_ajuste_resample = resample(df_task, svm_ajuste, desc_outer, store_models = TRUE )

rae <- svm_ajuste_resample$aggregate(msr("regr.rae"))
print(round(rae,3))

```

El RAE para el caso del SVM es de $0.742$ con hiper-parámetros, un error demasiado alto comparado con el caso sin ajuste de hiper-parámetros.

## 3.4. Métodos de ensembles con y sin ajuste de hiper-parámetros

Los métodos de ensambles que vamos a estudiar son Random Forest y Gradient Boosting.

Como hemos encontrado dos tipos de learner para cada método y vamos a calcular el RAE con ambos. Nos quedaremos con ranger y xgboost para ajustar hiper-parámetos como pide la práctica.

### Sin ajuste

```{r}

set.seed(100430523)

t <- matrix(0,1,2)
colnames(t)=c("Learner","Rae")

metodos <- c("regr.ranger","regr.randomForest","regr.xgboost","regr.gbm" ) 

df_task <- as_task_regr(df, target="salida")

res_desc=rsmp("custom")
res_desc$instantiate(df_task, 
                     train=list(1:(6*365)), 
                     test=list((6*365+1):(9*365)))

preproceso <- po("imputesample") %>>% po("imputemedian") %>>% po("scalerange") 

for (i in metodos) {
  learner = lrn(i)
  graph = preproceso %>>% po(learner)
  secuencia = as_learner(graph)
  metodos_resample = resample(task=df_task, 
                             learner=secuencia, 
                             resampling=res_desc)
  rae = metodos_resample$aggregate(msr("regr.rae"))
  t <- rbind(t, c(i,rae))
}

t <- data.frame(t[-1,], row.names = NULL) 
t$Rae <- round(as.numeric(t$Rae),3)
t

```

### Con ajuste

Vamos a ajustar solo los dos hiper-parámetros más impontantes de cada modelo.

Random Forest con ranger: 

```{r message=TRUE, warning=FALSE}
set.seed(100430523)

ranger_space = ps(
  regr.ranger.mtry = p_int(lower=1, upper=10),
  regr.ranger.max.depth = p_int(lower=2, upper=10)
  
)
terminator = trm("evals", n_evals = 10)
tuner = tnr("random_search")
secuencia = preproceso %>>% po(lrn("regr.ranger"))

ranger_ajuste = AutoTuner$new(
  learner = secuencia,
  resampling = desc_inner,
  measure = msr("regr.rae"),
  search_space = ranger_space,
  terminator = terminator,
  tuner = tuner
)
ranger_ajuste_resample = resample(df_task, ranger_ajuste, desc_outer )

rae=ranger_ajuste_resample$aggregate(msr("regr.rae"))
print(rae)

```

El RAE con ranger para Random Forest es de: $0.383$

Gradient Boosting: 

```{r message=TRUE, warning=FALSE}
set.seed(100430523)

xgboost_space = ps(
  regr.xgboost.eta = p_dbl(lower=0.001, upper=0.6),
  regr.xgboost.max_depth = p_int(lower=1, upper=8),
  regr.xgboost.nrounds = p_int(lower=1, upper=8)
)
terminator = trm("evals", n_evals = 10)
tuner = tnr("random_search")
secuencia = preproceso %>>% po(lrn("regr.xgboost"))

xgboost_ajuste = AutoTuner$new(
  learner = secuencia,
  resampling = desc_inner,
  measure = msr("regr.rae"),
  search_space = xgboost_space,
  terminator = terminator,
  tuner = tuner
)
xgboost_ajuste_resample = resample(df_task, xgboost_ajuste, desc_outer )

rae=xgboost_ajuste_resample$aggregate(msr("regr.rae"))
print(rae)

```

El RAE con Gradient Boosting es de: $0.393$

# 4. Modelo final

Como hemos demostrado en todos los apartados anteriores, los dos métodos con menor RAE son cubist (entre los métodos no ensambles) y randomforest sin ajuste de hiper-parámetros (entre los métodos de ensambles).

Entre todos los modelos, en el que obtenemos un menor RAE y por tanto es el mejor modelo para procesar nuestros datos es: **cubist**. Con un RAE de $0.361$

Una vez escogido este mejor modelo, vamos a estimar el error de dicho modelo con los años que habíamos reservado para test.

```{r}

set.seed(100430523)
df_task=as_task_regr(df, target="salida")

res_desc=rsmp("custom")
res_desc$instantiate(df_task, train=list(1:(9*365)), test=list((9*365+1):(12*365)))

secuencia= preproceso %>>% po(lrn("regr.cubist"))
secuencia = as_learner(secuencia)

final_resample = resample(task=df_task, 
                           learner=secuencia, 
                           resampling=res_desc)
rae=final_resample$aggregate(msr("regr.rae"))
print(rae)

```

El error estimado es de $0.384$.

```{r}

modelo_final <- secuencia$train(df_task)
saveRDS(modelo_final, "modelo_final.rds")

```


Por último usaremos este modelo para predecir los datos de los próximos años para los datos de competición.

```{r}

noms_compet <- colnames(datos_compet)
Nas_compet <- round(1 - skim(datos_compet)$complete_rate,3)
datos_compet_RC <- remove_constant(datos_compet, na.rm = T, quiet = F)
datos_compet_RC_RNA <- datos_compet_RC[,!(names(datos_compet_RC) %in% noms_compet[Nas_compet >= 0.8])]

df2 <- datos_compet_RC_RNA

charcol <- c(colnames(df2[, sapply(df2, class) == 'character']))

for (i in charcol){
  df2[[i]] <- as.factor(df2[[i]])
}

c(colnames(df2[, sapply(df2, class) == 'character'])) #Ya no hay columnas que sea 'character', solo factores

source("extras_from_mlr.R")
df2 <- createDummyFeatures(df2, target = "salida")

set.seed(100430523)

predicciones_compet <- modelo_final$predict_newdata(df2)
write.csv(predicciones_compet$response,"predicciones_compet_9.csv", row.names = FALSE)

```


# 5. Hyperband

Tanto las técnicas vistas anteriormente como Grid Search y Random Search, como ésta de la que estamos hablando, que es Hyperband, se tratan de métodos de optimización de hiper-parámetros. El ajuste de hiper-parámetros lo planteamos como un problema de optimización, y consiste en encontrar una configuración de hiper-parámetros para un determinado modelo, que permita obtener el mayor rendimiento posible de la función que modeliza un conjunto de validación, a través precisamente de esos hiper-parámtros.

La diferencia fundamental entre Grid y Random Search con Hyperband, es que los dos primeros modelos buscan a ciegas entre el espacio de búsqueda de hipér-parámetros. Grid discretiza la búsqueda obteniendo todas las configuraciones posibles de hiper-parámetros y eligiendo la mejor, lo que hace que pueda ser muy elevado el número de configuraciones que tiene que calcular; mientras que con Random Search eliges aleatoriamente el número de configuraciones que quieres estudiar, lo que compensa el anterior problema. Ambos métodos son considerados dentro del grupo Black-Box Optimization.

Por su parte, Hyperband, la ventaja que tiene respecto a los dos anteriores métodos, es que se desempeña relativamente bien en conjuntos de datos con alta dimensionalidad, como es nuestro caso. Hyperband es como si llevara a cabo Grid Search, pero para varios valores de n. Para ello, Hyperband emplea un presupuesto mínimo asociado con cada valor de n, que se asignará a todas las configuraciones antes de que se descarten algunas. Por lo que, un valor de n mayor, implica una r menor y, un early stopping más agresivo (se descartarán más configuraciones en menor tiempo).

Vamos a utilizar Hyperband para ajustar los hiper-parámetros del método xgboost y lo compararemos con el realizado en la práctica anteriormente con Random Search.

https://peepdata.github.io/hyperparameter-optimization-hyperband/ 


```{r}
set.seed(100430523)

learner = lrn("regr.xgboost",
  nrounds           = to_tune(p_int(27, 243, tags = "budget")),
  eta               = to_tune(1e-4, 1, logscale = TRUE),
  max_depth         = to_tune(1, 20),
  colsample_bytree  = to_tune(1e-1, 1),
  colsample_bylevel = to_tune(1e-1, 1),
  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),
  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),
  subsample         = to_tune(1e-1, 1)
)
secuencia = preproceso %>>% learner

instance = tune(
  method = "hyperband",
  task = df_task,
  learner = secuencia,
  resampling = desc_inner,
  measures = msr("regr.rae"),
  eta = 3
)

instance$result

```

El error estimado para Hyperband es de: $0.385$, que mejora el anterior ajuste con Random Search para el Gradient Boosting.

\pagebreak

# Conclusión

Como hemos visto anteriormente, de entre todos los modelos estudiados y analizados, el mejor modelo para procesar nuestros datos, que es el modelo final elegido para predecir los datos de los próximos años (los datos de competición), ha sido el **Cubist** con imputesample, imputemedian y scalerange, porque con estos obteníamos el menor RAE, que era de $0.361$. Para los datos de test el RAE final es de $0.384$.


\pagebreak

# Bibliografía





